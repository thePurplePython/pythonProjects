{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-On Machine Learning with Scikit-Learn & TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAPTER 12: Distributing TensorFlow Across Devices and Servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed TensorFlow:\n",
    "-  provides full control how to split or replicate computation graph across GPUs/CPUs and servers\n",
    "-  ability to parallelize and synchronize operations via flexible parallelization approaches\n",
    "-  **note** => single machine GPU performance often just as fast as distributed machine GPU performance (**depends on network transfer speed too**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation / Setup:\n",
    "-  https://developer.nvidia.com/cuda-gpus\n",
    "-  http://max-likelihood.com/2016/06/18/aws-tensorflow-setup/\n",
    "-  http://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### must download and install appropriate version of CUDA (Compute Unified Device Architecture) and cuDNN (CUDA Deep Neural Network) libraries as well as must setup environment variables for TF to find CUDA and cuDNN because TF uses CUDA and cuDNN to control GPU cards and computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA (Compute Unified Device Architecture):\n",
    "-  https://developer.nvidia.com/cuda-zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuDNN (CUDA Deep Neural Network):\n",
    "-  https://developer.nvidia.com/cudnn\n",
    "-  apart of NVIDIA's Deep Learning SDK\n",
    "-  provides common DNN computations including activation layers, normalization, forward/backward convolutions, and pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Service Subscription:\n",
    "-  Amazon Web Services\n",
    "-  Microsoft Azure\n",
    "-  Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Simple Placer:\n",
    "-  _simple placer_ is used to place nodes that are not assigned to devices yet ...\n",
    "    -  if a node was already placed on a device in a previous run of the graph, it is left on that device.\n",
    "    -  else, if the user pinned a node to a device (described next), the placer places it on that device.\n",
    "    -  else, it defaults to GPU #0, or the CPU if there is no GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Execution:\n",
    "-  via CPU:\n",
    "    -  CPU operation's queue are dispatched to a thread pool (**inter-op thread pool**)\n",
    "    -  performs operations in parallel if CPU has multiple cores\n",
    "    -  some operations have multithreaded CPU kernels [**tasks are split into sub-operations**] and dispatched to 2nd thread pool (**intra-op thread pool**)\n",
    "    -  multiple operations and sub-operations have the ability to be performed in parallel on different CPU cores\n",
    "    -  control number of threads in inter-op pool => **inter_op_parallelism_threads**\n",
    "    -  control number of threads in intra-op pool => **intra_op_parallelism_threads**\n",
    "-  via GPU:\n",
    "    -  GPU operations queue are evaluated sequentially\n",
    "    -  many operations have multithreaded GPU kernels [CUDA and cuDNN]\n",
    "    -  thread pools trigger as many GPU thread pools as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Cluster:\n",
    "-  http://download.tensorflow.org/paper/whitepaper2015.pdf\n",
    "-  must define a _cluster_ to execute a tf graph across multiple tf servers possibly within multiple machines:\n",
    "    -  tf servers are called _tasks_\n",
    "    -  each _task_ belongs to a _job_\n",
    "    -  a _job_ is a named group of tasks with a common role (i.e. parameter server; worker)\n",
    "    -  a _client_ is a session with a process on any machine\n",
    "    -  tf servers provide two services (_master_ and _worker_):\n",
    "        -  _master_:\n",
    "            -  allow _clients_ to open sessions to run graphs\n",
    "            -  coordinates computations across _tasks_\n",
    "        -  _worker_:\n",
    "            -  executes computations on _tasks_\n",
    "    -  tf _client_ session can connect to multiple servers by opening multiple sessions in different threads\n",
    "    -  can run one _client_ per _task_ or just one _client_ to control all _tasks_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Management and Resource Containers:\n",
    "-  _local session_ => not distributed and each variable is managed by the session itself; variables are lost when session ends\n",
    "-  _distributed sessions_ => variables are managed by _resource containers_ located on cluster\n",
    "-  _resource containers_ make it easy to share variables across sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Communication via TF Queues:\n",
    "-  synchronous => first system sends a message to the second system and waits for a response\n",
    "-  asynchronous => first system does not wait for a response from the second system\n",
    "-  _queues_:\n",
    "    -  exchange data between multiple sessions\n",
    "    -  _async queue_ example => **client creates a graph that loads training data pushed to _queue_ while another client creates a graph that pulls data from the _queue_ to train a model**\n",
    "-  First-In First-Out (**FIFO**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enqueuing & Dequeuing:\n",
    "-  enqueue => push data to a queue\n",
    "-  dequeue => pull data out of a queuue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Load Management:\n",
    "-  transfering data from _filesystem to client to master task to other tasks_ is inefficient and can cause too much stress on FS / network bandwidth\n",
    "-  **solutions**:\n",
    "    -  _preloading_ => load training data in memory / assign to variable (**if data volumne fits in RAM**)\n",
    "    -  _reader operations_ => read training data directly from FS (**if data volumne does not fit in RAM**):\n",
    "        -  file formats:\n",
    "            -  CSV\n",
    "            -  fixed-length binary records\n",
    "            -  TFRecords\n",
    "            -  TextLineReader API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelizing Neural Networks on a TensorFlow Cluster:\n",
    "-  One Neural Network per Device:\n",
    "    -  solid use case for hyperparameter tuning => **each device in the cluster trains a different model with set of hyperparameters**\n",
    "    -  solid use case for ensemble learning => **each device in the cluster trains a different ensemble**\n",
    "-  Model Parallelism:\n",
    "    -  distributing model training into separate chunks on different devices\n",
    "    -  ***tricky setup and depends on architecture ... also not too beneficial***\n",
    "-  Data Parallelism:\n",
    "    -  replicate training on each device (_use different mini-batch on each device synchronously and asynchronously_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Implementation:\n",
    "-  _In Graph Replication + Synchronous Updates_:\n",
    "    -  With in-graph replication + synchronous updates, you build one big graph contain‐ ing all the model replicas (placed on different devices), and a few nodes to aggregate all their gradients and feed them to an optimizer. Your code opens a session to the cluster and simply runs the training operation repeatedly. - __Aurelien Geron [Hands on ML w SKLearn & TF]__\n",
    "-  _In Graph Replication + Asynchronous Updates_:\n",
    "    -  With in-graph replication + asynchronous updates, you also create one big graph, but with one optimizer per replica, and you run one thread per replica, repeatedly run‐ ning the replica’s optimizer. - __Aurelien Geron [Hands on ML w SKLearn & TF]__\n",
    "-  _Between Graph Replication + Synchronous Updates_:\n",
    "    -  With between-graph replication + asynchronous updates, you run multiple inde‐ pendent clients (typically in separate processes), each training the model replica as if it were alone in the world, but the parameters are actually shared with other replicas (using a resource container). - __Aurelien Geron [Hands on ML w SKLearn & TF]__\n",
    "-  _Between Graph Replication + Asynchronous Updates_:\n",
    "    -  With between-graph replication + synchronous updates, once again you run multiple clients, each training a model replica based on shared parameters, but this time you wrap the optimizer (e.g., a MomentumOptimizer) within a SyncReplicasOptimizer. Each replica uses this optimizer as it would use any other optimizer, but under the hood this optimizer sends the gradients to a set of queues (one per variable), which is read by one of the replica’s SyncReplicasOptimizer, called the chief. The chief aggre‐ gates the gradients and applies them, then writes a token to a token queue for each replica, signaling it that it can go ahead and compute the next gradients. This approach supports having spare replicas. - __Aurelien Geron [Hands on ML w SKLearn & TF]__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Exercises_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "0.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Devices [CPUs/GPUs] on a Single Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify cuda is installed properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### install tf w/ gpu support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source env/bin/activate\n",
    "# pip3 install -upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify tf w/ cuda+cudnn integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### managing gpu ram:\n",
    "-  **Errors**:\n",
    "    -  by default Tensorflow consumes all RAM in in **all** available GPUs the first time running a graph\n",
    "    -  cannot start a second Tensorflow program while first graph is running\n",
    "-  **Solutions**:\n",
    "    -  run each process on different GPU cards\n",
    "    -  consume only a fraction of the memory available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_ERROR_OUT_OF_MEMORY => error when trying to run multiple graphs on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_VISIBLE_DEVICES=0,1 python3 program_1.py => program_1 only runs on GPU cards 0 and 1\n",
    "# CURA_VISIBLE_DEVICES=3,2 python3 program_2.py => program_2 only runs on GPU cards 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4 # consumes only 40% of RAM\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pinning nodes onto devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    a = tf.Variable(3.0)\n",
    "    b = tf.constant(4.0)\n",
    "c = a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logging node placements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True # prints log message where node as been placed\n",
    "sess = tf.Session(config=config)\n",
    "a.initializer.run(session=sess)\n",
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### automated node placements via udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variables_on_cpu(op):\n",
    "    if op.type == \"Variable\":\n",
    "        return \"/cpu:0\"\n",
    "    else:\n",
    "        return \"/gpu:0\"\n",
    "\n",
    "with tf.device(variables_on_cpu):\n",
    "    a = tf.Variable(3.0)\n",
    "    b = tf.constant(4.0)\n",
    "    c = a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernels => implementation for a device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with tf.device(\"/gpu:0\"):\n",
    "#     i = tf.Variable(3)\n",
    "# sess.run(i.initializer) => will error if device does not have kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soft placement => have tf fall back to a CPU rather than GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device(\"/gpu:0\"):\n",
    "#     i = tf.Variable(3)\n",
    "# config = tf.ConfigProto()\n",
    "# config.allow_soft_placement = True # ... use CPU instead of GPU\n",
    "# sess = tf.Session(config=config)\n",
    "# sess.run(i.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### control dependencies:\n",
    "-  ex => postpone operations in graph to save RAM until time when operations require RAM to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1.0)\n",
    "b = a + 2.0\n",
    "with tf.control_dependencies([a,b]):\n",
    "    x = tf.constant(3.0)\n",
    "    y = tf.constant(4.0)\n",
    "z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Devices [CPUs/GPUs] Across Multiple Servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cluster specification:\n",
    "-  most simple and recommended to run one task per machine\n",
    "-  several servers on one machine requires allocating GPU RAM accordingly to avoid consuming all GPU RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_spec = tf.train.ClusterSpec({\n",
    "    \"ps\": [\n",
    "        \"machine-a.example.com:2221\",  # /job:ps/task:0\n",
    "    ],\n",
    "    \"worker\": [\n",
    "        \"machine-a.example.com:2222\",  # /job:worker/task:0\n",
    "        \"machine-b.example.com:2222\",  # /job:worker/task:1\n",
    "    ]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start tf server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# server = tf.train.Server(cluster_spec, job_name=\"worker\", task_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### block all process except tf processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# server.join() # blocks until the server stops (i.e. never)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### open session:\n",
    "-  client session can be opened on any of the servers running on any machine\n",
    "-  client can use the _gRPC_ (Google Remote Procedure Call) protocol to communicate with the server\n",
    "-  _tf cluster may communicate with any server in the cluster hence appropriate ports must be open_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = tf.constant(1.0)\n",
    "# b = a + 2\n",
    "# c = a * 3\n",
    "# with tf.Session(\"grpc://machine-b.example.com:2222\") as sess: # master\n",
    "    # print(c.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pinning operations across tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device(\"/job:ps/task:0/cpu:0\"):\n",
    "    # a = tf.constant(1.0)\n",
    "# with tf.device(\"/job:worker/task:0/gpu:1\"):\n",
    "    # b = a + 2\n",
    "# c = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pinning \"sharding\" variables to servers:\n",
    "-  best practice for large models to store model parameters on a set of parameter servers\n",
    "-  _best to store parameters on CPUs and computation on GPUs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device(tf.train.replica_device_setter(ps_tasks=2)): # distributes param across tasks in round-robin method\n",
    "    # v1 = tf.Variable(1.0) # pinned to /job:ps/task:0\n",
    "    # v2 = tf.Variable(2.0) # pinned to /job:ps/task:1\n",
    "    # v3 = tf.Variable(3.0) # pinned to /job:ps/task:0\n",
    "    # v4 = tf.Variable(4.0) # pinned to /job:ps/task:1\n",
    "    # v5 = tf.Variable(5.0) # pinned to /job:ps/task:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device(tf.train.replica_device_setter(ps_tasks=2)):\n",
    "    # v1 = tf.Variable(1.0) # pinned to /job:ps/task:0 (+ defaults to /cpu:0)\n",
    "    # v2 = tf.Variable(2.0) # pinned to /job:ps/task:1 (+ defaults to /cpu:0)\n",
    "    # v3 = tf.Variable(3.0) # pinned to /job:ps/task:0 (+ defaults to /cpu:0)\n",
    "# s = v1 + v2 # pinned to /job:worker (+ defaults to task:0/gpu:0)\n",
    "# with tf.device(\"/gpu:1\"):\n",
    "    # p1 = 2 * s # pinned to /job:worker/gpu:1 (+ defaults to /task:0)\n",
    "    # with tf.device(\"/task:1\"):\n",
    "        # p2 = 3 * s # pinned to /job:worker/task:1/gpu:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variable and resource container management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_client.py\n",
    "#    import tensorflow as tf\n",
    "#    import sys\n",
    "#    x = tf.Variable(0.0, name=\"x\")\n",
    "#    increment_x = tf.assign(x, x + 1)\n",
    "# with tf.Session(sys.argv[1]) as sess:\n",
    "#    if sys.argv[2:]==[\"init\"]:\n",
    "#        sess.run(x.initializer)\n",
    "#    sess.run(increment_x)\n",
    "#    print(x.eval())\n",
    "\n",
    "# $ python3 simple_client.py grpc://machine-a.example.com:2222 init\n",
    "#   1.0\n",
    "# $ python3 simple_client.py grpc://machine-b.example.com:2222\n",
    "#   2.0\n",
    "\n",
    "### executing independent computations on same cluster\n",
    "\n",
    "# with tf.variable_scope(\"my_problem_1\"):\n",
    "#   [...] # Construction phase of problem 1\n",
    "# with tf.container(\"my_problem_1\"):\n",
    "#   [...] # Construction phase of problem 1\n",
    "\n",
    "### frees up all resources on container + closes all open sessions on server\n",
    "# tf.Session.reset(\"grpc://machine-a.example.com:2222\", [\"my_problem_1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fifo queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = tf.FIFOQueue(capacity=10, dtypes=[tf.float32], shapes=[[2]],\n",
    "                     name=\"q\", shared_name=\"shared_q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enqueuing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data_loader.py\n",
    "# import tensorflow as tf\n",
    "# q = tf.FIFOQueue(capacity=10, [...], shared_name=\"shared_q\")\n",
    "# training_instance = tf.placeholder(tf.float32, shape=[2])\n",
    "# enqueue = q.enqueue([training_instance])\n",
    "# with tf.Session(\"grpc://machine-a.example.com:2222\") as sess:\n",
    "    # sess.run(enqueue, feed_dict={training_instance: [1., 2.]})\n",
    "    # sess.run(enqueue, feed_dict={training_instance: [3., 4.]})\n",
    "    # sess.run(enqueue, feed_dict={training_instance: [5., 6.]})\n",
    "# [...]\n",
    "# training_instances = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "# enqueue_many = q.enqueue([training_instances])\n",
    "# with tf.Session(\"grpc://machine-a.example.com:2222\") as sess:\n",
    "    # sess.run(enqueue_many,\n",
    "             # feed_dict={training_instances: [[1., 2.], [3., 4.], [5., 6.]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dequeuing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.py\n",
    "# import tensorflow as tf\n",
    "# q = tf.FIFOQueue(capacity=10, [...], shared_name=\"shared_q\")\n",
    "# dequeue = q.dequeue()\n",
    "# with tf.Session(\"grpc://machine-a.example.com:2222\") as sess:\n",
    "    # print(sess.run(dequeue)) # [1., 2.]\n",
    "    # print(sess.run(dequeue)) # [3., 4.]\n",
    "    # print(sess.run(dequeue)) # [5., 6.]\n",
    "# [...]\n",
    "# batch_size = 2\n",
    "# dequeue_mini_batch= q.dequeue_many(batch_size)\n",
    "# with tf.Session(\"grpc://machine-a.example.com:2222\") as sess:\n",
    "    # print(sess.run(dequeue_mini_batch)) # [[1., 2.], [4., 5.]]\n",
    "    # print(sess.run(dequeue_mini_batch)) # blocked waiting for another instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tuple queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensors w/ various types and shapes\n",
    "q = tf.FIFOQueue(capacity=10, dtypes=[tf.int32, tf.float32], shapes=[[],[3,2]],\n",
    "                     name=\"q\", shared_name=\"shared_q\")\n",
    "\n",
    "a = tf.placeholder(tf.int32, shape=())\n",
    "b = tf.placeholder(tf.float32, shape=(3, 2))\n",
    "enqueue = q.enqueue((a, b))\n",
    "\n",
    "# with tf.Session([...]) as sess:\n",
    "    # sess.run(enqueue, feed_dict={a: 10, b:[[1., 2.], [3., 4.], [5., 6.]]})\n",
    "    # sess.run(enqueue, feed_dict={a: 11, b:[[2., 4.], [6., 8.], [0., 2.]]})\n",
    "    # sess.run(enqueue, feed_dict={a: 12, b:[[3., 6.], [9., 2.], [5., 8.]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dequeue_a, dequeue_b = q.dequeue()\n",
    "# with tf.Session([...]) as sess:\n",
    "    # a_val, b_val = sess.run([dequeue_a, dequeue_b])\n",
    "    # print(a_val) # 10\n",
    "    # print(b_val) # [[1., 2.], [3., 4.], [5., 6.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "dequeue_as, dequeue_bs = q.dequeue_many(batch_size)\n",
    "\n",
    "# with tf.Session([...]) as sess:\n",
    "    # a, b = sess.run([dequeue_a, dequeue_b])\n",
    "    # print(a) # [10, 11]\n",
    "    # print(b) # [[[1., 2.], [3., 4.], [5., 6.]], [[2., 4.], [6., 8.], [0., 2.]]]\n",
    "    # a, b = sess.run([dequeue_a, dequeue_b]) # blocked waiting for another pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### close queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_q = q.close()\n",
    "# with tf.Session([...]) as sess:\n",
    "    # [...]\n",
    "    # sess.run(close_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random shuffle queue:\n",
    "-  useful for shuffling training instances at each epoch during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = tf.RandomShuffleQueue(capacity=50, min_after_dequeue=10,\n",
    "                              dtypes=[tf.float32], shapes=[()],\n",
    "                              name=\"q\", shared_name=\"shared_q\")\n",
    "\n",
    "dequeue = q.dequeue_many(5)\n",
    "\n",
    "# with tf.Session([...]) as sess:\n",
    "    # print(sess.run(dequeue)) # [ 20. 15. 11. 12. 4.] (17 items left)\n",
    "    # print(sess.run(dequeue)) # [ 5. 13. 6. 0. 17.] (12 items left)\n",
    "    # print(sess.run(dequeue)) # 12 - 5 < 10: blocked waiting for 3 more instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding fifo queue:\n",
    "-  useful for variable length inputs like sequences of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = tf.PaddingFIFOQueue(capacity=50, dtypes=[tf.float32], shapes=[(None, None)],\n",
    "                            name=\"q\", shared_name=\"shared_q\")\n",
    "\n",
    "v = tf.placeholder(tf.float32, shape=(None, None))\n",
    "enqueue = q.enqueue([v])\n",
    "\n",
    "# with tf.Session([...]) as sess:\n",
    "    # sess.run(enqueue, feed_dict={v: [[1., 2.], [3., 4.], [5., 6.]]}) # 3*2\n",
    "    # sess.run(enqueue, feed_dict={v: [[1.]]}) # 1*1\n",
    "    # sess.run(enqueue, feed_dict={v: [[7., 8., 9., 5.], [6., 7., 8., 9.]]}) # 2*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set_init = tf.placeholder(tf.float32, shape=(None, n_features))\n",
    "# training_set = tf.Variable(training_set_init, trainable=False, collections=[],\n",
    "#                               name=\"training_set\")\n",
    "# with tf.Session([...]) as sess:\n",
    "    # data = [...] # load the training data from the datastore\n",
    "    # sess.run(training_set.initializer, feed_dict={training_set_init: data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reader (graph dedicated to reading training instances from CSV files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-32-fd13b5ae4973>:1: TextLineReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TextLineDataset`.\n"
     ]
    }
   ],
   "source": [
    "reader = tf.TextLineReader(skip_header_lines=1)\n",
    "\n",
    "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
    "filename = tf.placeholder(tf.string)\n",
    "enqueue_filename = filename_queue.enqueue([filename])\n",
    "close_filename_queue = filename_queue.close()\n",
    "\n",
    "key, value = reader.read(filename_queue)\n",
    "\n",
    "x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
    "features = tf.stack([x1, x2])\n",
    "\n",
    "# push training instances to shuffle queue\n",
    "instance_queue = tf.RandomShuffleQueue(\n",
    "        capacity=10, min_after_dequeue=2,\n",
    "        dtypes=[tf.float32, tf.int32], shapes=[[2],[]],\n",
    "        name=\"instance_q\", shared_name=\"shared_instance_q\")\n",
    "enqueue_instance = instance_queue.enqueue([features, target])\n",
    "close_instance_queue = instance_queue.close()\n",
    "\n",
    "# execute graph / iterate through file 1 row at a time until empty\n",
    "# with tf.Session([...]) as sess:\n",
    "    # sess.run(enqueue_filename, feed_dict={filename: \"my_test.csv\"})\n",
    "    # sess.run(close_filename_queue)\n",
    "# try:\n",
    "    # while True:\n",
    "        # sess.run(enqueue_instance)\n",
    "# except tf.errors.OutOfRangeError as ex:\n",
    "    # pass # no more records in the current file and no more files to read\n",
    "# sess.run(close_instance_queue)\n",
    "\n",
    "# instance_queue = tf.RandomShuffleQueue([...], shared_name=\"shared_instance_q\")\n",
    "# mini_batch_instances, mini_batch_targets = instance_queue.dequeue_up_to(2)\n",
    "# [...] # use the mini_batch instances and targets to build the training graph training_op = [...]\n",
    "# with tf.Session([...]) as sess:\n",
    "    # try:\n",
    "        # for step in range(max_steps):\n",
    "            # sess.run(training_op)\n",
    "# except tf.errors.OutOfRangeError as ex:\n",
    "    # pass # no more training instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multithreaded reader (higher throughput via multiple threads reading data simultaneuously via many readers):\n",
    "-  classes:\n",
    "    -  _Coordinator_ => coordinates stopping multiple threads\n",
    "    -  _QueueRunner_ => runs multiple threads each executing enqueue operations repeatedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = tf.train.Coordinator()\n",
    "\n",
    "# with not coord.should_stop():\n",
    "    # [...] do something\n",
    "\n",
    "coord.request_stop()\n",
    "# coord.join(list_of_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [...] # same construction phase as earlier\n",
    "# queue_runner = tf.train.QueueRunner(instance_queue, [enqueue_instance] * 5)\n",
    "# with tf.Session() as sess:\n",
    "    # sess.run(enqueue_filename, feed_dict={filename: \"my_test.csv\"})\n",
    "    # sess.run(close_filename_queue)\n",
    "# coord = tf.train.Coordinator()\n",
    "# enqueue_threads = queue_runner.create_threads(sess, coord=coord, start=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reading simultaneously from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_push_instance(filename_queue, instance_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    key, value = reader.read(filename_queue)\n",
    "    x1, x2, target = tf.decode_csv(value, record_defaults=[[-1.], [-1.], [-1]])\n",
    "    features = tf.stack([x1, x2])\n",
    "    enqueue_instance = instance_queue.enqueue([features, target])\n",
    "    return enqueue_instance\n",
    "\n",
    "# define queues\n",
    "filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string], shapes=[()])\n",
    "filename = tf.placeholder(tf.string)\n",
    "enqueue_filename = filename_queue.enqueue([filename])\n",
    "close_filename_queue = filename_queue.close()\n",
    "# instance_queue = tf.RandomShuffleQueue([...])\n",
    "\n",
    "# read_and_enqueue_ops = [\n",
    "    # read_and_push_instance(filename_queue, instance_queue)\n",
    "    # for i in range(5)]\n",
    "# queue_runner = tf.train.QueueRunner(instance_queue, read_and_enqueue_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional exercises:\n",
    "\n",
    "https://github.com/ageron/handson-ml/blob/master/12_distributed_tensorflow.ipynb\n",
    "\n",
    "1. If you get a CUDA_ERROR_OUT_OF_MEMORY when starting your TensorFlow pro‐ gram, what is probably going on? What can you do about it?\n",
    "2. What is the difference between pinning an operation on a device and placing an operation on a device?\n",
    "3. If you are running on a GPU-enabled TensorFlow installation, and you just use the default placement, will all operations be placed on the first GPU?\n",
    "4. If you pin a variable to \"/gpu:0\", can it be used by operations placed on /gpu:1? Or by operations placed on \"/cpu:0\"? Or by operations pinned to devices loca‐ ted on other servers?\n",
    "5. Can two operations placed on the same device run in parallel?\n",
    "6. What is a control dependency and when would you want to use one?\n",
    "7. Suppose you train a DNN for days on a TensorFlow cluster, and immediately after your training program ends you realize that you forgot to save the model using a Saver. Is your trained model lost?\n",
    "8. Train several DNNs in parallel on a TensorFlow cluster, using different hyper‐ parameter values. This could be DNNs for MNIST classification or any other task you are interested in. The simplest option is to write a single client program that trains only one DNN, then run this program in multiple processes in parallel, with different hyperparameter values for each client. The program should have command-line options to control what server and device the DNN should be placed on, and what resource container and hyperparameter values to use (make sure to use a different resource container for each DNN). Use a validation set or cross-validation to select the top three models.\n",
    "9. Create an ensemble using the top three models from the previous exercise. Define it in a single graph, ensuring that each DNN runs on a different device. Evaluate it on the validation set: does the ensemble perform better than the indi‐ vidual DNNs?\n",
    "10. Train a DNN using between-graph replication and data parallelism with asyn‐ chronous updates, timing how long it takes to reach a satisfying performance. Next, try again using synchronous updates. Do synchronous updates produce a better model? Is training faster? Split the DNN vertically and place each vertical slice on a different device, and train the model again. Is training any faster? Is the performance any different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
