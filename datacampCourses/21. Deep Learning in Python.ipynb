{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course: _Deep Learning in Python_:\n",
    "1.  deep learning and neural network basics\n",
    "2.  neural network optimization via backward propagation\n",
    "3.  deep learning models via keras (https://keras.io/)\n",
    "4.  tuning keras models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html\n",
    "-  https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html\n",
    "-  https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html\n",
    "-  https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html\n",
    "-  https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _1. Basics of Deep Learning and Neural Networks:_\n",
    "-  DL models capture interactions among features via DL functions\n",
    "-  High Level Architecture:\n",
    "    -  **Input Layer** => represents predictive features\n",
    "    -  **Hidden Layer(s)** => represents aggregation from input data to capture interactions among input layer features\n",
    "    -  **Output Layer** => represents target value predictions\n",
    "-  Forward Propagation:\n",
    "    -  passes feature information through network to make prediction in output layer\n",
    "    -  follows a multiply add process like the dot product\n",
    "    -  performs forward propagation for one data point at a time\n",
    "    -  output is the prediction for the specific data point\n",
    "    -  lines connect input node to hidden node w/ associated **_weight_** indicating how strongly input affects target hidden node\n",
    "    -  weights => **parameters that are tweaked when fitting neural network model to training dataset**\n",
    "-  Activation Functions:\n",
    "    -  essential for neural network ability of predictive power\n",
    "    -  **allows model to capture nonlinearity relationships within data**\n",
    "    -  function applied to node inputs to produce node output for each node\n",
    "    -  ReLU (rectified linear activation)\n",
    "-  Multiple Hidden Layers:\n",
    "    -  **deep networks interally learn and build representations of patterns in the data**\n",
    "    -  **neueral network receives weights that find relevant patterns in the data to make better predictions**'\n",
    "    -  model training process sets weights accordingly to optimize predictive accuracy\n",
    "    -  the last layers capture the most complex interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5]\n",
      "==========\n",
      "{'node_0': array([2, 4]), 'node_1': array([ 4, -5]), 'output': array([2, 7])}\n",
      "==========\n",
      "26\n",
      "==========\n",
      "-13\n",
      "==========\n",
      "[ 26 -13]\n",
      "==========\n",
      "-39\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_data = np.array([3, 5])\n",
    "print(input_data)\n",
    "print(\"=\"*10)\n",
    "\n",
    "weights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n",
    "print(weights)\n",
    "print(\"=\"*10)\n",
    "\n",
    "node_0_value = (input_data * weights[\"node_0\"]).sum()\n",
    "print(node_0_value) # (3 * 2) + (5 * 4) = 26\n",
    "print(\"=\"*10)\n",
    "\n",
    "node_1_value = (input_data * weights[\"node_1\"]).sum()\n",
    "print(node_1_value) # (3 * 4) + (5 * -5) = -13\n",
    "print(\"=\"*10)\n",
    "\n",
    "hidden_layer_outputs = np.array([node_0_value, node_1_value])\n",
    "print(hidden_layer_outputs)\n",
    "print(\"=\"*10)\n",
    "\n",
    "output = (hidden_layer_outputs * weights[\"output\"]).sum()\n",
    "print(output) # (26 * 2) + (-13 * 7) = -39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "26\n",
      "==========\n",
      "-13\n",
      "0\n",
      "==========\n",
      "[26  0]\n",
      "==========\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([3, 5])\n",
    "weights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n",
    "\n",
    "def relu(input):\n",
    "    '''\n",
    "    Define your relu activation function here\n",
    "    This function takes a single number as an input,\n",
    "    returning 0 if the input is negative,\n",
    "    and the input if the input is positive\n",
    "    '''\n",
    "    output = max(0, input)\n",
    "    return(output)\n",
    "\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "print(node_0_input) # (3 * 2) + (5 * 4) = 26\n",
    "print(node_0_output) # max(0, 26) => 26\n",
    "print(\"=\"*10)\n",
    "\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "print(node_1_input) # (3 * 4) + (5 * -5) = -13\n",
    "print(node_1_output) # max(0, -13) => 0\n",
    "print(\"=\"*10)\n",
    "\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "print(hidden_layer_outputs)\n",
    "print(\"=\"*10)\n",
    "\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "print(model_output) # (26 * 2) + (0 * 7) = 52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 63, 0, 148]\n"
     ]
    }
   ],
   "source": [
    "new_data = [np.array([3, 5]), np.array([ 1, -1]), np.array([0, 0]), np.array([8, 4])]\n",
    "weights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n",
    "\n",
    "def predict_with_network_single_layer(input_data_row, weights):\n",
    "\n",
    "    node_0_input = (input_data_row * weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    node_1_input = (input_data_row * weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    return(model_output)\n",
    "\n",
    "results = [] # store predicted results in empty list\n",
    "for i in new_data:\n",
    "    results.append(predict_with_network_single_layer(i, weights)) # append prediction to results\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multi-layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "26\n",
      "==========\n",
      "-13\n",
      "0\n",
      "==========\n",
      "[26  0]\n",
      "==========\n",
      "-26\n",
      "0\n",
      "==========\n",
      "26\n",
      "26\n",
      "==========\n",
      "[ 0 26]\n",
      "==========\n",
      "182\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([3, 5])\n",
    "weights = {'node_0_0': np.array([2, 4]),\n",
    " 'node_0_1': np.array([ 4, -5]),\n",
    " 'node_1_0': np.array([-1,  2]),\n",
    " 'node_1_1': np.array([1, 2]),\n",
    " 'output': np.array([2, 7])}\n",
    "\n",
    "def predict_with_network_multi_layers(input_data):\n",
    "    node_0_0_input = (input_data * weights['node_0_0']).sum() # (3 * 2) + (5 * 4) = 26\n",
    "    node_0_0_output = relu(node_0_0_input) # max(0, 26) => 26\n",
    "    print(node_0_0_input)\n",
    "    print(node_0_0_output)\n",
    "    print(\"=\"*10)\n",
    "    \n",
    "    node_0_1_input = (input_data * weights['node_0_1']).sum() # (3 * 4) + (5 * -5) = -13\n",
    "    node_0_1_output = relu(node_0_1_input) # max(0, -13) => 0\n",
    "    print(node_0_1_input)\n",
    "    print(node_0_1_output)\n",
    "    print(\"=\"*10)\n",
    "\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output]) # [26, -13]\n",
    "    print(hidden_0_outputs)\n",
    "    print(\"=\"*10)\n",
    "    \n",
    "    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum() # (26 * -1) + (0 * 2) = -26\n",
    "    node_1_0_output = relu(node_1_0_input) # max(0, -26) => 0\n",
    "    print(node_1_0_input)\n",
    "    print(node_1_0_output)\n",
    "    print(\"=\"*10)\n",
    "    \n",
    "    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum() # (0 * 2) + (26 * 1) = 26\n",
    "    node_1_1_output = relu(node_1_1_input) # max(0, 26) => 26\n",
    "    print(node_1_1_input)\n",
    "    print(node_1_1_output)\n",
    "    print(\"=\"*10)\n",
    "\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output]) # [0, 26]\n",
    "    print(hidden_1_outputs)\n",
    "    print(\"=\"*10)\n",
    "\n",
    "    model_output = (hidden_1_outputs * weights['output']).sum() # (0 * 2) + (26 * 7) = 182\n",
    "    \n",
    "    return(model_output)\n",
    "\n",
    "output = predict_with_network_multi_layers(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _2. Optimizing a Neural Network with Backward Propagation:_\n",
    "-  Loss Function:\n",
    "    -  aggregates errors in predictions from many data points into a single number\n",
    "    -  **measure of model's predictive performance**\n",
    "    -  lower loss function value means a better model\n",
    "    -  error => [predicted value - actual value]\n",
    "-  Gradient Descent \"array of slopes\":\n",
    "    -  **Slope** => describes the direction and steepness of the line\n",
    "    -  goal is to find weights that give the lowest value for the loss function\n",
    "    -  good to visualize slope descent via 2D plot => **_X axis (weight); Y axis (loss function)_**\n",
    "    -  High Level Process => _start at a random point and find slope (**derivative**) then take a step downhill until reaching optimal weight value_:\n",
    "        -  goal is to reach the minimum value of the slope curve via a flattened surface on the curve\n",
    "        -  to avoid too big of a step use the _**learning rate**_ * _**slope**_ to reach optimal weight value\n",
    "    -  **How to Calculate Slope for each data point/weight?** Ex - fed layer node (3) => weight (2) => output layer node prediction (6); actual target value (10):\n",
    "        -  **Slope of the loss function with respect to the value at the node feeding into:**\n",
    "            -  2 * (predicted value - actual value) = 2 * Error ... 2 * -4\n",
    "        -  **The value of the node that feeds into the weight:**\n",
    "            -  3\n",
    "            -  slope => 2 * -4 * 3 = -24\n",
    "            -  apply learning rate = 0.01\n",
    "            -  **new weight value => 2 - 0.01 * (-24) = 2.24**\n",
    "        -  The slope of the activation function with respect to the value feeding into:\n",
    "            -  ReLU => slope of 1 if positive; slope of 0 if negative\n",
    "-  Backpropagation:\n",
    "    -  calculates shopes to optimize more complex deep learning models\n",
    "    -  takes error from output layer backwards through hidden layers towards input layer\n",
    "    -  **trying to estimate the slope of the loss function with respect to each weight**\n",
    "    -  **Calculation process** Ex - output layer node prediction (7); actual target value (4) <= weights [1, 2] <= fed layer nodes [1, 3]:\n",
    "        -  Node value feeding into weight:\n",
    "            -  [1, 3]\n",
    "        -  Slope of loss function with respect to the node it feeds into:\n",
    "            -  loss function slope => 2 * 3 [error => 7-4] = 6\n",
    "        -  Slope of activation function at the node it feeds into:\n",
    "            -  top weight s1ope => 1 * 6 * 1 = 6\n",
    "            -  bottom weight slope => 3 * 6 * 1 = 18\n",
    "        -  **Hidden Layer Backpropagation** Ex - node output from previous forward layer [6]; weight [0], fed node weight [0]:\n",
    "            -  Value at the node feeding into the weight => 0\n",
    "            -  The slope of the activation function for the node being fed into => 0 if ReLU\n",
    "            -  The slope of the loss function with respect to the output node => 6\n",
    "            -  Slope => 0 * 0 * 6 = 0\n",
    "-  Forward / Backward Propagation Steps:\n",
    "    1.  start at random set of weights\n",
    "    2.  use forward propagation to make prediction\n",
    "    3.  use backward propagation to calculate slope of loss function with respect to each weight\n",
    "    4.  multiply slope by learning rate and subtract from current weight to get optimal adjusted weight\n",
    "    5.  continue this process until a flat value is determined\n",
    "-  Stochastic Gradient Descent \"batch gradient descent\":\n",
    "    -  calculate slopes on only subset of the data aka batch\n",
    "    -  use a different batch of data to calculate the next update\n",
    "    -  complete the process all over again once all data is used\n",
    "-  Epoch:\n",
    "    -  **each time running through the entire training dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weight changes test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "input_data = np.array([0, 3])\n",
    "\n",
    "weights_0 = {'node_0': [2, 1],\n",
    "             'node_1': [1, 2],\n",
    "             'output': [1, 1]\n",
    "            }\n",
    "\n",
    "target_actual = 3\n",
    "\n",
    "model_output_0 = predict_with_network_single_layer(input_data, weights_0)\n",
    "error_0 = model_output_0 - target_actual # predicted minus actual\n",
    "\n",
    "# create weights that cause the network to make perfect prediction => 3\n",
    "weights_1 = {'node_0': [2, 1],\n",
    "             'node_1': [1, 0],\n",
    "             'output': [1, 1]\n",
    "            }\n",
    "\n",
    "model_output_1 = predict_with_network_single_layer(input_data, weights_1)\n",
    "error_1 = model_output_1 - target_actual # predicted minus actual\n",
    "\n",
    "print(error_0)\n",
    "print(error_1) # perfect prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error with weights_0: 37.500000\n",
      "Mean squared error with weights_1: 49.890625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "input_data = [np.array([0, 3]), np.array([1, 2]), np.array([-1, -2]), np.array([4, 0])]\n",
    "target_actuals = [1, 3, 5, 7]\n",
    "weights_0 = {'node_0': np.array([2, 1]), 'node_1': np.array([1, 2]), 'output': np.array([1, 1])} # model 0\n",
    "weights_1 = {'node_0': np.array([2, 1]), 'node_1': np.array([1. , 1.5]), 'output': np.array([1. , 1.5])} # model 1\n",
    "\n",
    "model_output_0 = [] # empty list for model 0 prediction values\n",
    "model_output_1 = [] # empty list for model 1 prediction values\n",
    "\n",
    "for row in input_data: # iterate over each row of input_data\n",
    "    model_output_0.append(predict_with_network_single_layer(row, weights_0))\n",
    "    model_output_1.append(predict_with_network_single_layer(row, weights_1))\n",
    "\n",
    "mse_0 = mean_squared_error(target_actuals, model_output_0)\n",
    "mse_1 = mean_squared_error(target_actuals, model_output_1)\n",
    "\n",
    "print(\"Mean squared error with weights_0: %f\" %mse_0)\n",
    "print(\"Mean squared error with weights_1: %f\" %mse_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "==========\n",
      "-7\n",
      "==========\n",
      "[-14 -28 -42]\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0, 2, 1])\n",
    "input_data = np.array([1, 2, 3])\n",
    "target = 0\n",
    "\n",
    "preds = (weights * input_data).sum()\n",
    "print(preds)\n",
    "print(\"=\"*10)\n",
    "\n",
    "error = target - preds\n",
    "print(error)\n",
    "print(\"=\"*10)\n",
    "\n",
    "slope = 2 * input_data * error # 2 * x * (y-xb)\n",
    "print(slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### improve model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5.04\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "preds = (weights * input_data).sum()\n",
    "error = preds - target\n",
    "slope = 2 * input_data * error\n",
    "\n",
    "weights_updated = weights - learning_rate * slope # update weights\n",
    "preds_updated = (weights_updated * input_data).sum() # updated preds\n",
    "error_updated = preds_updated - target # updated error\n",
    "\n",
    "print(error) # original error\n",
    "print(error_updated) # updated error ... it decreased!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### error udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(input_data, target, weights):\n",
    "    preds = (weights * input_data).sum()\n",
    "    error = preds - target\n",
    "    return(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_error(input_data, target, weights):\n",
      "    preds = (weights * input_data).sum()\n",
      "    error = preds - target\n",
      "    return(error)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(get_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### slope udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slope(input_data, target, weights):\n",
    "    error = get_error(input_data, target, weights)\n",
    "    slope = 2 * input_data * error\n",
    "    return(slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_slope(input_data, target, weights):\n",
      "    error = get_error(input_data, target, weights)\n",
      "    slope = 2 * input_data * error\n",
      "    return(slope)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(get_slope))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mse udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse(input_data, target, weights):\n",
    "    errors = get_error(input_data, target, weights)\n",
    "    mse = np.mean(errors**2)\n",
    "    return(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_mse(input_data, target, weights):\n",
      "    errors = get_error(input_data, target, weights)\n",
      "    mse = np.mean(errors**2)\n",
      "    return(mse)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(get_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### improve multiple weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.990700200532531e-05\n",
      "2.587178983956073e-05\n",
      "1.3411935852823729e-05\n",
      "6.952747546105929e-06\n",
      "3.604304327902831e-06\n",
      "1.8684713635843904e-06\n",
      "9.686155548823403e-07\n",
      "5.02130303650741e-07\n",
      "2.6030434941282503e-07\n",
      "1.349417747356346e-07\n",
      "6.995381602286371e-08\n",
      "3.6264058226252544e-08\n",
      "1.8799287784552644e-08\n",
      "9.74555078743844e-09\n",
      "5.052093528195461e-09\n",
      "2.6190052850101636e-09\n",
      "1.357692339744687e-09\n",
      "7.038277089184618e-10\n",
      "3.6486428432233195e-10\n",
      "1.8914564499147536e-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_data = np.array([1, 2, 3])\n",
    "weights = np.array([-0.49929916, 1.00140168, -0.49789747])\n",
    "\n",
    "n_updates = 20\n",
    "mse_hist = []\n",
    "\n",
    "for i in range(n_updates): # iterate over the number of updates\n",
    "    slope = get_slope(input_data, target, weights)\n",
    "    weights = weights - 0.01 * slope # update weights\n",
    "    mse = get_mse(input_data, target, weights)\n",
    "    print(mse)\n",
    "    mse_hist.append(mse)\n",
    "\n",
    "plt.plot(mse_hist)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _3. Building Deep Learning Models with Keras_:\n",
    "-  Model Building Steps:\n",
    "    1.  Specify Architecture [# of layers, # of nodes (columns) in each layer, activation function, etc.]\n",
    "    2.  Compile [loss function, optimization details (learning rate), etc.]\n",
    "    3.  Fit [cycle of backpropagation/gradient descent => optimization of model weights w/ data]\n",
    "    4.  Predict [make predictions (save model, reload model, apply model)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regression - prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(534,)\n",
      "(534, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wage_per_hour</th>\n",
       "      <th>union</th>\n",
       "      <th>education_yrs</th>\n",
       "      <th>experience_yrs</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>marr</th>\n",
       "      <th>south</th>\n",
       "      <th>manufacturing</th>\n",
       "      <th>construction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.10</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.95</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.67</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.00</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.50</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   wage_per_hour  union  education_yrs  experience_yrs  age  female  marr  \\\n",
       "0           5.10      0              8              21   35       1     1   \n",
       "1           4.95      0              9              42   57       1     1   \n",
       "2           6.67      0             12               1   19       0     0   \n",
       "3           4.00      0             12               4   22       0     0   \n",
       "4           7.50      0             12              17   35       0     1   \n",
       "\n",
       "   south  manufacturing  construction  \n",
       "0      0              1             0  \n",
       "1      0              1             0  \n",
       "2      0              1             0  \n",
       "3      0              0             0  \n",
       "4      0              0             0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = '/Users/grp/Documents/BIGDATA/DATACAMP/pythonCourses/21 - deepinglearningwithpython/hourly_wages.csv'\n",
    "\n",
    "df = pd.read_csv(path) # read csv\n",
    "\n",
    "target = df.loc[:, \"wage_per_hour\"] # target var\n",
    "target = target.values # convert to matrix\n",
    "print(target.shape)\n",
    "\n",
    "'''\n",
    "# another way to get features cols and target col via pandas\n",
    "X = df.drop(columns=['wage_per_hour']).values\n",
    "y = df['wage_per_hour'].values\n",
    "'''\n",
    "\n",
    "predictors = df.drop([\"wage_per_hour\"], axis=1) # drop target var\n",
    "predictors = predictors.values # convert to matrix\n",
    "print(predictors.shape)\n",
    "\n",
    "df.head() # original df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 1 => regression - specify model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "n_cols = predictors.shape[1] # number of cols in predictors matrix\n",
    "print(n_cols)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_cols,))) # 1st layer\n",
    "# n_cols => (9 items in each row of data, 'any # of rows of data are acceptable as inputs') or param input_dim = 9\n",
    "model.add(Dense(32, activation='relu')) # 2nd layer\n",
    "model.add(Dense(1)) # output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2 => regression - compile the model:\n",
    "-  loss function (https://keras.io/losses/)\n",
    "-  optimization (https://keras.io/optimizers/#adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: mean_squared_error\n"
     ]
    }
   ],
   "source": [
    "# opt => adam ... industry standard; loss => industry standard for regression\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "print(\"Loss function: \" + model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step3 => regression - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "534/534 [==============================] - 0s 37us/step - loss: 21.3077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d60e160>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classification - prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 2)\n",
      "(891, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>male</th>\n",
       "      <th>age_was_missing</th>\n",
       "      <th>embarked_from_cherbourg</th>\n",
       "      <th>embarked_from_queenstown</th>\n",
       "      <th>embarked_from_southampton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass   age  sibsp  parch     fare  male  age_was_missing  \\\n",
       "0         0       3  22.0      1      0   7.2500     1            False   \n",
       "1         1       1  38.0      1      0  71.2833     0            False   \n",
       "2         1       3  26.0      0      0   7.9250     0            False   \n",
       "3         1       1  35.0      1      0  53.1000     0            False   \n",
       "4         0       3  35.0      0      0   8.0500     1            False   \n",
       "\n",
       "   embarked_from_cherbourg  embarked_from_queenstown  \\\n",
       "0                        0                         0   \n",
       "1                        1                         0   \n",
       "2                        0                         0   \n",
       "3                        0                         0   \n",
       "4                        0                         0   \n",
       "\n",
       "   embarked_from_southampton  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.utils import to_categorical # OHE\n",
    "\n",
    "path = '/Users/grp/Documents/BIGDATA/DATACAMP/pythonCourses/21 - deepinglearningwithpython/titanic_all_numeric.csv'\n",
    "\n",
    "df = pd.read_csv(path) # read csv\n",
    "\n",
    "target = to_categorical(df[\"survived\"]) # 2 cols (binary); 1 or 0\n",
    "print(target.shape)\n",
    "\n",
    "predictors = df.drop([\"survived\"], axis=1) # drop target var\n",
    "predictors = predictors.values # convert to matrix\n",
    "target = to_categorical(df.survived)\n",
    "\n",
    "print(predictors.shape)\n",
    "\n",
    "df.head() # original df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 1 => classification - specify model architecture:\n",
    "-  output layer has separate node for each possible class outcome hence use \"softmax\" activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "n_cols = predictors.shape[1] # number of cols in predictors matrix\n",
    "print(n_cols)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(2, activation='softmax')) # 2 output nodes for binary target classes (1 or 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step2 => classification - compile the model:\n",
    "-  loss function => \"categorical_corssentropy\" similar to \"log loss\" in that lower value is better (https://keras.io/losses/)\n",
    "-  optimization (https://keras.io/optimizers/#sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt => sgd; loss => industry standard for classification\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step3 => classification - fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "891/891 [==============================] - 0s 30us/step - loss: 0.5955 - acc: 0.7003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d8e42b0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step4 => classification - make predictions:\n",
    "-  save / reload / apply model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 10)\n"
     ]
    }
   ],
   "source": [
    "pred_data = np.array([[2, 34.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "       [2, 31.0, 1, 1, 26.25, 0, False, 0, 0, 1],\n",
    "       [1, 11.0, 1, 2, 120.0, 1, False, 0, 0, 1],\n",
    "       [3, 0.42, 0, 1, 8.5167, 1, False, 1, 0, 0],\n",
    "       [3, 27.0, 0, 0, 6.975, 1, False, 0, 0, 1],\n",
    "       [3, 31.0, 0, 0, 7.775, 1, False, 0, 0, 1],\n",
    "       [1, 39.0, 0, 0, 0.0, 1, False, 0, 0, 1],\n",
    "       [3, 18.0, 0, 0, 7.775, 0, False, 0, 0, 1],\n",
    "       [2, 39.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "       [1, 33.0, 1, 0, 53.1, 0, False, 0, 0, 1],\n",
    "       [3, 26.0, 0, 0, 7.8875, 1, False, 0, 0, 1],\n",
    "       [3, 39.0, 0, 0, 24.15, 1, False, 0, 0, 1],\n",
    "       [2, 35.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "       [3, 6.0, 4, 2, 31.275, 0, False, 0, 0, 1],\n",
    "       [3, 30.5, 0, 0, 8.05, 1, False, 0, 0, 1],\n",
    "       [1, 29.69911764705882, 0, 0, 0.0, 1, True, 0, 0, 1],\n",
    "       [3, 23.0, 0, 0, 7.925, 0, False, 0, 0, 1],\n",
    "       [2, 31.0, 1, 1, 37.0042, 1, False, 1, 0, 0],\n",
    "       [3, 43.0, 0, 0, 6.45, 1, False, 0, 0, 1],\n",
    "       [3, 10.0, 3, 2, 27.9, 1, False, 0, 0, 1],\n",
    "       [1, 52.0, 1, 1, 93.5, 0, False, 0, 0, 1],\n",
    "       [3, 27.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "       [1, 38.0, 0, 0, 0.0, 1, False, 0, 0, 1],\n",
    "       [3, 27.0, 0, 1, 12.475, 0, False, 0, 0, 1],\n",
    "       [3, 2.0, 4, 1, 39.6875, 1, False, 0, 0, 1],\n",
    "       [3, 29.69911764705882, 0, 0, 6.95, 1, True, 0, 1, 0],\n",
    "       [3, 29.69911764705882, 0, 0, 56.4958, 1, True, 0, 0, 1],\n",
    "       [2, 1.0, 0, 2, 37.0042, 1, False, 1, 0, 0],\n",
    "       [3, 29.69911764705882, 0, 0, 7.75, 1, True, 0, 1, 0],\n",
    "       [1, 62.0, 0, 0, 80.0, 0, False, 0, 0, 0],\n",
    "       [3, 15.0, 1, 0, 14.4542, 0, False, 1, 0, 0],\n",
    "       [2, 0.83, 1, 1, 18.75, 1, False, 0, 0, 1],\n",
    "       [3, 29.69911764705882, 0, 0, 7.2292, 1, True, 1, 0, 0],\n",
    "       [3, 23.0, 0, 0, 7.8542, 1, False, 0, 0, 1],\n",
    "       [3, 18.0, 0, 0, 8.3, 1, False, 0, 0, 1],\n",
    "       [1, 39.0, 1, 1, 83.1583, 0, False, 1, 0, 0],\n",
    "       [3, 21.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "       [3, 29.69911764705882, 0, 0, 8.05, 1, True, 0, 0, 1],\n",
    "       [3, 32.0, 0, 0, 56.4958, 1, False, 0, 0, 1],\n",
    "       [1, 29.69911764705882, 0, 0, 29.7, 1, True, 1, 0, 0],\n",
    "       [3, 20.0, 0, 0, 7.925, 1, False, 0, 0, 1],\n",
    "       [2, 16.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "       [1, 30.0, 0, 0, 31.0, 0, False, 1, 0, 0],\n",
    "       [3, 34.5, 0, 0, 6.4375, 1, False, 1, 0, 0],\n",
    "       [3, 17.0, 0, 0, 8.6625, 1, False, 0, 0, 1],\n",
    "       [3, 42.0, 0, 0, 7.55, 1, False, 0, 0, 1],\n",
    "       [3, 29.69911764705882, 8, 2, 69.55, 1, True, 0, 0, 1],\n",
    "       [3, 35.0, 0, 0, 7.8958, 1, False, 1, 0, 0],\n",
    "       [2, 28.0, 0, 1, 33.0, 1, False, 0, 0, 1],\n",
    "       [1, 29.69911764705882, 1, 0, 89.1042, 0, True, 1, 0, 0],\n",
    "       [3, 4.0, 4, 2, 31.275, 1, False, 0, 0, 1],\n",
    "       [3, 74.0, 0, 0, 7.775, 1, False, 0, 0, 1],\n",
    "       [3, 9.0, 1, 1, 15.2458, 0, False, 1, 0, 0],\n",
    "       [1, 16.0, 0, 1, 39.4, 0, False, 0, 0, 1],\n",
    "       [2, 44.0, 1, 0, 26.0, 0, False, 0, 0, 1],\n",
    "       [3, 18.0, 0, 1, 9.35, 0, False, 0, 0, 1],\n",
    "       [1, 45.0, 1, 1, 164.8667, 0, False, 0, 0, 1],\n",
    "       [1, 51.0, 0, 0, 26.55, 1, False, 0, 0, 1],\n",
    "       [3, 24.0, 0, 3, 19.2583, 0, False, 1, 0, 0],\n",
    "       [3, 29.69911764705882, 0, 0, 7.2292, 1, True, 1, 0, 0],\n",
    "       [3, 41.0, 2, 0, 14.1083, 1, False, 0, 0, 1],\n",
    "       [2, 21.0, 1, 0, 11.5, 1, False, 0, 0, 1],\n",
    "       [1, 48.0, 0, 0, 25.9292, 0, False, 0, 0, 1],\n",
    "       [3, 29.69911764705882, 8, 2, 69.55, 0, True, 0, 0, 1],\n",
    "       [2, 24.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "       [2, 42.0, 0, 0, 13.0, 0, False, 0, 0, 1],\n",
    "       [2, 27.0, 1, 0, 13.8583, 0, False, 1, 0, 0],\n",
    "       [1, 31.0, 0, 0, 50.4958, 1, False, 0, 0, 1],\n",
    "       [3, 29.69911764705882, 0, 0, 9.5, 1, True, 0, 0, 1],\n",
    "       [3, 4.0, 1, 1, 11.1333, 1, False, 0, 0, 1],\n",
    "       [3, 26.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "       [1, 47.0, 1, 1, 52.5542, 0, False, 0, 0, 1],\n",
    "       [1, 33.0, 0, 0, 5.0, 1, False, 0, 0, 1],\n",
    "       [3, 47.0, 0, 0, 9.0, 1, False, 0, 0, 1],\n",
    "       [2, 28.0, 1, 0, 24.0, 0, False, 1, 0, 0],\n",
    "       [3, 15.0, 0, 0, 7.225, 0, False, 1, 0, 0],\n",
    "       [3, 20.0, 0, 0, 9.8458, 1, False, 0, 0, 1],\n",
    "       [3, 19.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "       [3, 29.69911764705882, 0, 0, 7.8958, 1, True, 0, 0, 1],\n",
    "       [1, 56.0, 0, 1, 83.1583, 0, False, 1, 0, 0],\n",
    "       [2, 25.0, 0, 1, 26.0, 0, False, 0, 0, 1],\n",
    "       [3, 33.0, 0, 0, 7.8958, 1, False, 0, 0, 1],\n",
    "       [3, 22.0, 0, 0, 10.5167, 0, False, 0, 0, 1],\n",
    "       [2, 28.0, 0, 0, 10.5, 1, False, 0, 0, 1],\n",
    "       [3, 25.0, 0, 0, 7.05, 1, False, 0, 0, 1],\n",
    "       [3, 39.0, 0, 5, 29.125, 0, False, 0, 1, 0],\n",
    "       [2, 27.0, 0, 0, 13.0, 1, False, 0, 0, 1],\n",
    "       [1, 19.0, 0, 0, 30.0, 0, False, 0, 0, 1],\n",
    "       [3, 29.69911764705882, 1, 2, 23.45, 0, True, 0, 0, 1],\n",
    "       [1, 26.0, 0, 0, 30.0, 1, False, 1, 0, 0],\n",
    "       [3, 32.0, 0, 0, 7.75, 1, False, 0, 1, 0]], dtype=object)\n",
    "\n",
    "print(pred_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "if not os.path.exists('persist_models'):\n",
    "        os.mkdir('persist_models')\n",
    "\n",
    "model.save('/Users/grp/datacampNotebooks/pythonCourses/persist_models/keras_model.h5')\n",
    "ann_model = load_model('/Users/grp/datacampNotebooks/pythonCourses/persist_models/keras_model.h5')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras_model.h5\r\n"
     ]
    }
   ],
   "source": [
    "! ls /Users/grp/datacampNotebooks/pythonCourses/persist_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7501191  0.24988091]\n",
      " [0.55431104 0.44568905]\n",
      " [0.3261089  0.6738911 ]\n",
      " [0.47804806 0.52195203]\n",
      " [0.8097411  0.19025898]\n",
      " [0.8247998  0.17520025]\n",
      " [0.9083324  0.09166753]\n",
      " [0.63209397 0.36790606]\n",
      " [0.788      0.21200001]\n",
      " [0.4672823  0.5327177 ]]\n",
      "==========\n",
      "[0.24988091 0.44568905 0.6738911  0.52195203 0.19025898 0.17520025\n",
      " 0.09166753 0.36790606 0.21200001 0.5327177  0.209872   0.306088\n",
      " 0.2040975  0.5022912  0.18186449 0.11937501 0.32262376 0.5044144\n",
      " 0.10174178 0.46371743 0.5789555  0.21406886 0.09541091 0.32589003\n",
      " 0.48001966 0.16640602 0.514202   0.58520097 0.17635098 0.61582077\n",
      " 0.47506103 0.48877853 0.2171238  0.23140137 0.27934408 0.6004549\n",
      " 0.26034522 0.16639715 0.5243199  0.46129996 0.25616112 0.348135\n",
      " 0.52163583 0.19058137 0.2940976  0.11517151 0.39868942 0.20560941\n",
      " 0.4983697  0.61064804 0.48033163 0.03306286 0.5199829  0.553543\n",
      " 0.37582904 0.33383563 0.7455716  0.33510885 0.42135727 0.2171238\n",
      " 0.13508384 0.27876326 0.43768921 0.43063316 0.31834844 0.27118203\n",
      " 0.4432422  0.5054476  0.18404989 0.48415834 0.20999248 0.5187386\n",
      " 0.16332157 0.10662639 0.46239763 0.4640595  0.28949594 0.26389104\n",
      " 0.1647369  0.6153531  0.5033216  0.1646403  0.3826096  0.2569751\n",
      " 0.20484306 0.31241032 0.30999997 0.5215156  0.35844952 0.5104639\n",
      " 0.18346812]\n"
     ]
    }
   ],
   "source": [
    "predictions = ann_model.predict(pred_data) # predict new data; output will be class [0, 1] probas\n",
    "print(predictions[:10]) # class [0,1] pred probas\n",
    "\n",
    "print(\"=\"*10)\n",
    "\n",
    "predicted_prob_true = predictions[:,1] # filter out probas of True [i.e. = 1] classes [2nd col/index pos 1]\n",
    "print(predicted_prob_true) # class = True (1) pred probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 32)                352       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 418\n",
      "Trainable params: 418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "ann_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _4. Fine-Tuning Keras Models_:\n",
    "-  Stochastic Gradient Descent:\n",
    "    -  uses a fixed learning rate\n",
    "-  Validation Dataset:\n",
    "    -  used to test model performance\n",
    "    -  cross validation usually not needed because DL models train large volumns of data so validation split is efficient\n",
    "-  Early Stopping:\n",
    "    -  stop taining after amount of epochs stop improving model's validation loss\n",
    "-  Model Capacity:\n",
    "    -  model capacity is model's ability to capture predictive patterns in data\n",
    "    -  **validation score is ultimate measure of a model's predictive quality**\n",
    "    -  recommendation:\n",
    "        -  start with a small network\n",
    "        -  get the validation score\n",
    "        -  keep increasing capacity (units aka nodes, layers) until validation score is no longer improving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### changing optimization parameters [learning rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (10,)\n",
    "\n",
    "def get_new_model(input_shape = input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_new_model(input_shape = input_shape):\n",
      "    model = Sequential()\n",
      "    model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
      "    model.add(Dense(100, activation='relu'))\n",
      "    model.add(Dense(2, activation='softmax'))\n",
      "    return(model)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(get_new_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Testing model with learning rate: 0.000001\n",
      "\n",
      "Epoch 1/1\n",
      "891/891 [==============================] - 0s 439us/step - loss: 0.8160\n",
      "\n",
      "\n",
      "Testing model with learning rate: 0.010000\n",
      "\n",
      "Epoch 1/1\n",
      "891/891 [==============================] - 0s 472us/step - loss: 2.2980\n",
      "\n",
      "\n",
      "Testing model with learning rate: 1.000000\n",
      "\n",
      "Epoch 1/1\n",
      "891/891 [==============================] - 0s 488us/step - loss: 9.6095\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "lr_to_test = [.000001, 0.01, 1]\n",
    "\n",
    "for lr in lr_to_test: # iterate over learning rates\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    model = get_new_model()\n",
    "    my_optimizer = SGD(lr=lr)\n",
    "    model.compile(optimizer = my_optimizer, loss = 'categorical_crossentropy')\n",
    "    model.fit(predictors, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classification - evaluate model accuracy on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/10\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 1.0869 - acc: 0.5859 - val_loss: 0.6226 - val_acc: 0.7090\n",
      "Epoch 2/10\n",
      "623/623 [==============================] - 0s 51us/step - loss: 0.6597 - acc: 0.6533 - val_loss: 0.6409 - val_acc: 0.7090\n",
      "Epoch 3/10\n",
      "623/623 [==============================] - 0s 57us/step - loss: 0.7040 - acc: 0.6517 - val_loss: 0.5373 - val_acc: 0.7313\n",
      "Epoch 4/10\n",
      "623/623 [==============================] - 0s 49us/step - loss: 0.6546 - acc: 0.6629 - val_loss: 0.6865 - val_acc: 0.6455\n",
      "Epoch 5/10\n",
      "623/623 [==============================] - 0s 52us/step - loss: 0.5981 - acc: 0.6838 - val_loss: 0.5520 - val_acc: 0.7612\n",
      "Epoch 6/10\n",
      "623/623 [==============================] - 0s 51us/step - loss: 0.6016 - acc: 0.7047 - val_loss: 0.5266 - val_acc: 0.7239\n",
      "Epoch 7/10\n",
      "623/623 [==============================] - 0s 52us/step - loss: 0.5777 - acc: 0.6950 - val_loss: 0.5362 - val_acc: 0.7313\n",
      "Epoch 8/10\n",
      "623/623 [==============================] - 0s 50us/step - loss: 0.6407 - acc: 0.6742 - val_loss: 0.4913 - val_acc: 0.7612\n",
      "Epoch 9/10\n",
      "623/623 [==============================] - 0s 52us/step - loss: 0.5979 - acc: 0.7143 - val_loss: 0.5268 - val_acc: 0.7351\n",
      "Epoch 10/10\n",
      "623/623 [==============================] - 0s 49us/step - loss: 0.5803 - acc: 0.7127 - val_loss: 0.4771 - val_acc: 0.7537\n"
     ]
    }
   ],
   "source": [
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "print(input_shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(predictors, target, validation_split=0.3, epochs=10) # validation set is reported in each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "Train on 623 samples, validate on 268 samples\n",
      "Epoch 1/30\n",
      "623/623 [==============================] - 1s 1ms/step - loss: 1.2280 - acc: 0.5778 - val_loss: 0.6088 - val_acc: 0.7090\n",
      "Epoch 2/30\n",
      "623/623 [==============================] - 0s 52us/step - loss: 0.8496 - acc: 0.6228 - val_loss: 0.5891 - val_acc: 0.7127\n",
      "Epoch 3/30\n",
      "623/623 [==============================] - 0s 52us/step - loss: 0.7031 - acc: 0.6324 - val_loss: 0.5170 - val_acc: 0.7500\n",
      "Epoch 4/30\n",
      "623/623 [==============================] - 0s 52us/step - loss: 0.6140 - acc: 0.6742 - val_loss: 0.5506 - val_acc: 0.7500\n",
      "Epoch 5/30\n",
      "623/623 [==============================] - 0s 50us/step - loss: 0.6062 - acc: 0.6854 - val_loss: 0.6483 - val_acc: 0.7313\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1305dd6d8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "n_cols = predictors.shape[1]\n",
    "input_shape = (n_cols,)\n",
    "print(input_shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_shape = input_shape))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# patience = stop optimization when the validation loss hasn't improved for 2 epochs\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "model.fit(predictors, target, validation_split=0.3, epochs=30, callbacks = [early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "\n",
    "model_1 = Sequential() # red\n",
    "model_2 = Sequential() # blue; outputs a lower log loss\n",
    "\n",
    "model_1.add(Dense(10, activation='relu', input_shape=input_shape))\n",
    "model_1.add(Dense(10, activation='relu'))\n",
    "\n",
    "model_2.add(Dense(100, activation='relu', input_shape=input_shape))\n",
    "model_2.add(Dense(100, activation='relu'))\n",
    "\n",
    "model_1.add(Dense(2, activation='softmax'))\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model_1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "model_2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_1_training = model_1.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "model_2_training = model_2.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_72 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 242\n",
      "Trainable params: 242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "==========\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_74 (Dense)             (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 11,402\n",
      "Trainable params: 11,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_1.summary())\n",
    "print(\"=\"*10)\n",
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add layers to a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_shape = (n_cols,)\n",
    "\n",
    "model_1 = Sequential() # red\n",
    "model_2 = Sequential() # blue; outputs a lower log loss\n",
    "\n",
    "model_1.add(Dense(50, activation='relu', input_shape=input_shape))\n",
    "\n",
    "model_2.add(Dense(50, activation='relu', input_shape = input_shape))\n",
    "model_2.add(Dense(50, activation='relu', input_shape = input_shape))\n",
    "model_2.add(Dense(50, activation='relu', input_shape = input_shape))\n",
    "\n",
    "model_1.add(Dense(2, activation='softmax'))\n",
    "model_2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model_1.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "model_2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_1_training = model_1.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "model_2_training = model_2.fit(predictors, target, epochs=20, validation_split=0.4, callbacks=[early_stopping_monitor], verbose=False)\n",
    "\n",
    "plt.plot(model_1_training.history['val_loss'], 'r', model_2_training.history['val_loss'], 'b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_114 (Dense)            (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 652\n",
      "Trainable params: 652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "==========\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_115 (Dense)            (None, 50)                550       \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 5,752\n",
      "Trainable params: 5,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model_1.summary())\n",
    "print(\"=\"*10)\n",
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### digit recognition model:\n",
    "-  GPU help => https://www.datacamp.com/community/tutorials/deep-learning-jupyter-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...    775    776   777  \\\n",
       "0    5    0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  ...  0.608  0.609  0.61   \n",
       "1    4    0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000  0.000  0.00   \n",
       "2    3    0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000  0.000  0.00   \n",
       "3    0    0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000  0.000  0.00   \n",
       "4    2    0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.000  0.000  0.00   \n",
       "\n",
       "     778    779    780    781    782    783    784  \n",
       "0  0.611  0.612  0.613  0.614  0.615  0.616  0.617  \n",
       "1  0.000  0.000  0.000  0.000  0.000  0.000  0.000  \n",
       "2  0.000  0.000  0.000  0.000  0.000  0.000  0.000  \n",
       "3  0.000  0.000  0.000  0.000  0.000  0.000  0.000  \n",
       "4  0.000  0.000  0.000  0.000  0.000  0.000  0.000  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/grp/Documents/BIGDATA/DATACAMP/pythonCourses/21 - deepinglearningwithpython/mnist.csv\",\\\n",
    "                header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2001, 784)\n",
      "(2001, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "X = df.drop(columns=[df.columns[0]])\n",
    "y = to_categorical(df[df.columns[0]])\n",
    "print(X.shape) # 28 * 28 = 784 features\n",
    "print(y.shape) # digit labels => 0, 1, 2, 3, 4, 5, 6, 7, 8, 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 601 samples\n",
      "Epoch 1/100\n",
      "1400/1400 [==============================] - 3s 2ms/step - loss: 11.6104 - acc: 0.2507 - val_loss: 10.3182 - val_acc: 0.3428\n",
      "Epoch 2/100\n",
      "1400/1400 [==============================] - 0s 104us/step - loss: 9.3354 - acc: 0.4007 - val_loss: 8.4637 - val_acc: 0.4509\n",
      "Epoch 3/100\n",
      "1400/1400 [==============================] - 0s 104us/step - loss: 8.1951 - acc: 0.4643 - val_loss: 8.0967 - val_acc: 0.4709\n",
      "Epoch 4/100\n",
      "1400/1400 [==============================] - 0s 105us/step - loss: 7.4587 - acc: 0.5214 - val_loss: 8.0155 - val_acc: 0.4842\n",
      "Epoch 5/100\n",
      "1400/1400 [==============================] - 0s 108us/step - loss: 6.9715 - acc: 0.5536 - val_loss: 6.5969 - val_acc: 0.5757\n",
      "Epoch 6/100\n",
      "1400/1400 [==============================] - 0s 106us/step - loss: 6.4274 - acc: 0.5800 - val_loss: 6.5221 - val_acc: 0.5774\n",
      "Epoch 7/100\n",
      "1400/1400 [==============================] - 0s 105us/step - loss: 5.8586 - acc: 0.6193 - val_loss: 5.6920 - val_acc: 0.6256\n",
      "Epoch 8/100\n",
      "1400/1400 [==============================] - 0s 108us/step - loss: 5.4618 - acc: 0.6443 - val_loss: 5.4844 - val_acc: 0.6356\n",
      "Epoch 9/100\n",
      "1400/1400 [==============================] - 0s 107us/step - loss: 5.1406 - acc: 0.6657 - val_loss: 5.6935 - val_acc: 0.6190\n",
      "Epoch 10/100\n",
      "1400/1400 [==============================] - 0s 105us/step - loss: 4.7421 - acc: 0.6850 - val_loss: 4.5118 - val_acc: 0.6988\n",
      "Epoch 11/100\n",
      "1400/1400 [==============================] - 0s 106us/step - loss: 4.0092 - acc: 0.7336 - val_loss: 3.9762 - val_acc: 0.7304\n",
      "Epoch 12/100\n",
      "1400/1400 [==============================] - 0s 106us/step - loss: 3.4607 - acc: 0.7700 - val_loss: 3.4831 - val_acc: 0.7604\n",
      "Epoch 13/100\n",
      "1400/1400 [==============================] - 0s 104us/step - loss: 3.2517 - acc: 0.7857 - val_loss: 3.9375 - val_acc: 0.7321\n",
      "Epoch 14/100\n",
      "1400/1400 [==============================] - 0s 112us/step - loss: 3.2222 - acc: 0.7836 - val_loss: 3.4464 - val_acc: 0.7687\n",
      "Epoch 15/100\n",
      "1400/1400 [==============================] - 0s 112us/step - loss: 3.0761 - acc: 0.7950 - val_loss: 3.6607 - val_acc: 0.7571\n",
      "Epoch 16/100\n",
      "1400/1400 [==============================] - 0s 105us/step - loss: 3.0609 - acc: 0.8000 - val_loss: 3.8967 - val_acc: 0.7471\n",
      "Epoch 17/100\n",
      "1400/1400 [==============================] - 0s 108us/step - loss: 3.0775 - acc: 0.7957 - val_loss: 3.3206 - val_acc: 0.7737\n",
      "Epoch 18/100\n",
      "1400/1400 [==============================] - 0s 107us/step - loss: 2.7806 - acc: 0.8186 - val_loss: 3.1518 - val_acc: 0.7937\n",
      "Epoch 19/100\n",
      "1400/1400 [==============================] - 0s 109us/step - loss: 2.8872 - acc: 0.8100 - val_loss: 3.4722 - val_acc: 0.7671\n",
      "Epoch 20/100\n",
      "1400/1400 [==============================] - 0s 104us/step - loss: 2.7827 - acc: 0.8214 - val_loss: 3.0226 - val_acc: 0.7937\n",
      "Epoch 21/100\n",
      "1400/1400 [==============================] - 0s 105us/step - loss: 2.7677 - acc: 0.8193 - val_loss: 3.5815 - val_acc: 0.7637\n",
      "Epoch 22/100\n",
      "1400/1400 [==============================] - 0s 105us/step - loss: 2.7424 - acc: 0.8214 - val_loss: 2.9739 - val_acc: 0.8037\n",
      "Epoch 23/100\n",
      "1400/1400 [==============================] - 0s 111us/step - loss: 2.5742 - acc: 0.8321 - val_loss: 3.0832 - val_acc: 0.7920\n",
      "Epoch 24/100\n",
      "1400/1400 [==============================] - 0s 113us/step - loss: 2.6144 - acc: 0.8279 - val_loss: 3.0564 - val_acc: 0.7987\n",
      "Epoch 25/100\n",
      "1400/1400 [==============================] - 0s 109us/step - loss: 2.5972 - acc: 0.8321 - val_loss: 3.0105 - val_acc: 0.8020\n",
      "Epoch 26/100\n",
      "1400/1400 [==============================] - 0s 111us/step - loss: 2.5359 - acc: 0.8314 - val_loss: 3.0778 - val_acc: 0.7987\n",
      "Epoch 27/100\n",
      "1400/1400 [==============================] - 0s 113us/step - loss: 2.6543 - acc: 0.8286 - val_loss: 3.3362 - val_acc: 0.7787\n",
      "Epoch 28/100\n",
      "1400/1400 [==============================] - 0s 111us/step - loss: 2.6327 - acc: 0.8279 - val_loss: 2.9943 - val_acc: 0.8020\n",
      "Epoch 29/100\n",
      "1400/1400 [==============================] - 0s 102us/step - loss: 2.4577 - acc: 0.8393 - val_loss: 3.0858 - val_acc: 0.7937\n",
      "Epoch 30/100\n",
      "1400/1400 [==============================] - 0s 104us/step - loss: 2.5159 - acc: 0.8379 - val_loss: 3.1555 - val_acc: 0.7870\n",
      "Epoch 31/100\n",
      "1400/1400 [==============================] - 0s 111us/step - loss: 2.4903 - acc: 0.8400 - val_loss: 3.2263 - val_acc: 0.7787\n",
      "Epoch 32/100\n",
      "1400/1400 [==============================] - 0s 114us/step - loss: 2.6633 - acc: 0.8271 - val_loss: 3.5692 - val_acc: 0.7554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1588869e8>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(50, activation='relu', input_shape = (784,)))\n",
    "model.add(Dense(25, activation='relu', input_shape = (784,)))\n",
    "model.add(Dense(10, activation='softmax')) # 10 since there are 10 possible outputs [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=10)\n",
    "# Fit the model\n",
    "model.fit(X, y, epochs=100, validation_split=0.3, callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
